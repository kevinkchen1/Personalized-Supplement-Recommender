{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c44de27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Fetching supplement URLs...\n",
      "==================================================\n",
      "Fetching supplement links from main page...\n",
      "Found 28 supplement links\n",
      "\n",
      "Found supplements:\n",
      "  - Acidophilus: https://www.mayoclinic.org/drugs-supplements-acidophilus/art-20361967\n",
      "  - Aloe: https://www.mayoclinic.org/drugs-supplements-aloe/art-20362267\n",
      "  - Coenzyme Q10: https://www.mayoclinic.org/drugs-supplements-coenzyme-q10/art-20362602\n",
      "  - Creatine: https://www.mayoclinic.org/drugs-supplements-creatine/art-20347591\n",
      "  - DHEA: https://www.mayoclinic.org/drugs-supplements-dhea/art-20364199\n",
      "  ... and 23 more\n",
      "\n",
      "==================================================\n",
      "Step 2: Testing with single supplement (Melatonin):\n",
      "==================================================\n",
      "Scraping: Melatonin\n",
      "\n",
      "Sample data structure:\n",
      "name: Melatonin\n",
      "url: https://www.mayoclinic.org/drugs-supplements-melatonin/art-20363071\n",
      "display_name: Melatonin\n",
      "category: Other Supplement\n",
      "overview: Melatonin is a hormone in your body that plays a role in sleep. The production and release of melato...\n",
      "research_findings: list with 0 items\n",
      "safety_rating: dict with 2 items\n",
      "side_effects: list with 7 items\n",
      "interactions: list with 10 items\n",
      "contraindications: list with 1 items\n",
      "usage_notes: Your body likely produces enough melatonin for its general needs. However, evidence suggests that me...\n",
      "\n",
      "==================================================\n",
      "Step 3: Scraping all supplements...\n",
      "==================================================\n",
      "\n",
      "[1/28] Scraping: Acidophilus\n",
      "[2/28] Scraping: Aloe\n",
      "[3/28] Scraping: Coenzyme Q10\n",
      "[4/28] Scraping: Creatine\n",
      "[5/28] Scraping: DHEA\n",
      "[6/28] Scraping: Evening primrose\n",
      "[7/28] Scraping: Fish oil\n",
      "[8/28] Scraping: Flaxseed and flaxseed oil\n",
      "[9/28] Scraping: Folate (folic acid)\n",
      "[10/28] Scraping: Ginkgo\n",
      "[11/28] Scraping: Glucosamine\n",
      "[12/28] Scraping: Honey\n",
      "[13/28] Scraping: L-arginine\n",
      "[14/28] Scraping: Marijuana\n",
      "[15/28] Scraping: Melatonin\n",
      "[16/28] Scraping: Milk thistle\n",
      "[17/28] Scraping: Niacin\n",
      "[18/28] Scraping: Red yeast rice\n",
      "[19/28] Scraping: SAMe\n",
      "[20/28] Scraping: St. John's wort\n",
      "[21/28] Scraping: Tea tree oil\n",
      "[22/28] Scraping: Vitamin A\n",
      "[23/28] Scraping: Vitamin B-6\n",
      "[24/28] Scraping: Vitamin B-12\n",
      "[25/28] Scraping: Vitamin C\n",
      "[26/28] Scraping: Vitamin D\n",
      "[27/28] Scraping: Vitamin E\n",
      "[28/28] Scraping: Zinc\n",
      "\n",
      "Successfully scraped 28/28 supplements\n",
      "Data saved to mayo_clinic_supplements_raw.csv\n",
      "\n",
      "First few rows:\n",
      "           name          category  \\\n",
      "0   Acidophilus  Other Supplement   \n",
      "1          Aloe  Other Supplement   \n",
      "2  Coenzyme Q10  Other Supplement   \n",
      "3      Creatine  Other Supplement   \n",
      "4          DHEA  Other Supplement   \n",
      "\n",
      "                                       safety_rating  \n",
      "0  {'rating': 'Green Light - Generally safe', 'ex...  \n",
      "1  {'rating': 'Green Light - Generally safe', 'ex...  \n",
      "2  {'rating': 'Green Light - Generally safe', 'ex...  \n",
      "3  {'rating': 'Green Light - Generally safe', 'ex...  \n",
      "4  {'rating': 'Avoid', 'explanation': 'While some...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "class MayoClinicScraper:\n",
    "    \"\"\"\n",
    "    Web scraper for Mayo Clinic supplements information.\n",
    "    Extracts supplement details including safety info, interactions, and evidence.\n",
    "    \"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://www.mayoclinic.org\"\n",
    "    SUPPLEMENTS_PAGE = \"https://www.mayoclinic.org/drugs-supplements\"\n",
    "    \n",
    "    def __init__(self, delay: float = 2.0):\n",
    "        \"\"\"\n",
    "        Initialize scraper with politeness delay between requests.\n",
    "        \n",
    "        Args:\n",
    "            delay: Seconds to wait between requests (default 2.0)\n",
    "        \"\"\"\n",
    "        self.delay = delay\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "        self.supplement_urls = {}\n",
    "    \n",
    "    def get_supplement_links(self) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Scrape the main supplements page to get all supplement URLs with correct IDs.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping supplement names to their full URLs\n",
    "        \"\"\"\n",
    "        print(\"Fetching supplement links from main page...\")\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(self.SUPPLEMENTS_PAGE, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find the \"Herbs, supplements and vitamins\" section\n",
    "            supplement_section = soup.find('h2', string=re.compile(r'Herbs, supplements and vitamins', re.I))\n",
    "            \n",
    "            if not supplement_section:\n",
    "                print(\"Warning: Could not find supplements section\")\n",
    "                return {}\n",
    "            \n",
    "            # Find all links in the list after this heading\n",
    "            supplement_links = {}\n",
    "            current = supplement_section.find_next('ul')\n",
    "            \n",
    "            if current:\n",
    "                for link in current.find_all('a', href=True):\n",
    "                    href = link['href']\n",
    "                    name = link.get_text(strip=True)\n",
    "                    \n",
    "                    # Only include links that match the pattern\n",
    "                    if '/drugs-supplements-' in href and '/art-' in href:\n",
    "                        full_url = href if href.startswith('http') else self.BASE_URL + href\n",
    "                        supplement_links[name] = full_url\n",
    "            \n",
    "            print(f\"Found {len(supplement_links)} supplement links\")\n",
    "            self.supplement_urls = supplement_links\n",
    "            return supplement_links\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching supplement links: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def scrape_supplement(self, supplement_name: str, url: str = None) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Scrape data for a single supplement.\n",
    "        \n",
    "        Args:\n",
    "            supplement_name: Display name of supplement (e.g., 'Melatonin')\n",
    "            url: Full URL to scrape (optional, uses stored URL if available)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing supplement data or None if scraping fails\n",
    "        \"\"\"\n",
    "        if url is None:\n",
    "            if supplement_name in self.supplement_urls:\n",
    "                url = self.supplement_urls[supplement_name]\n",
    "            else:\n",
    "                print(f\"No URL found for {supplement_name}. Run get_supplement_links() first.\")\n",
    "                return None\n",
    "        \n",
    "        print(f\"Scraping: {supplement_name}\")\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            data = {\n",
    "                'name': supplement_name,\n",
    "                'url': url,\n",
    "                'display_name': self._extract_display_name(soup, supplement_name),\n",
    "                'category': self._extract_category(soup),\n",
    "                'overview': self._extract_overview(soup),\n",
    "                'research_findings': self._extract_research(soup),\n",
    "                'safety_rating': self._extract_safety_rating(soup),\n",
    "                'side_effects': self._extract_side_effects(soup),\n",
    "                'interactions': self._extract_interactions(soup),\n",
    "                'contraindications': self._extract_contraindications(soup),\n",
    "                'usage_notes': self._extract_usage_notes(soup)\n",
    "            }\n",
    "            \n",
    "            return data\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error scraping {supplement_name}: {e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error for {supplement_name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _extract_display_name(self, soup: BeautifulSoup, fallback: str) -> str:\n",
    "        \"\"\"Extract the display name of the supplement.\"\"\"\n",
    "        # Try h1 tag first\n",
    "        h1 = soup.find('h1')\n",
    "        if h1:\n",
    "            return h1.get_text(strip=True)\n",
    "        \n",
    "        # Fallback to title tag\n",
    "        title = soup.find('title')\n",
    "        if title:\n",
    "            title_text = title.get_text(strip=True)\n",
    "            # Remove \"- Mayo Clinic\" suffix if present\n",
    "            return title_text.split('-')[0].strip()\n",
    "        \n",
    "        # Final fallback: format the URL name\n",
    "        return fallback.replace('-', ' ').title()\n",
    "    \n",
    "    def _extract_category(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Determine supplement category (vitamin, mineral, herb, etc.).\"\"\"\n",
    "        text = soup.get_text().lower()\n",
    "        \n",
    "        if 'vitamin' in text[:500]:\n",
    "            return 'Vitamin'\n",
    "        elif 'mineral' in text[:500]:\n",
    "            return 'Mineral'\n",
    "        elif any(word in text[:500] for word in ['herb', 'botanical', 'plant']):\n",
    "            return 'Herb'\n",
    "        elif 'amino acid' in text[:500]:\n",
    "            return 'Amino Acid'\n",
    "        elif 'hormone' in text[:500]:\n",
    "            return 'Hormone'\n",
    "        else:\n",
    "            return 'Other Supplement'\n",
    "    \n",
    "    def _extract_overview(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Extract overview/description section.\"\"\"\n",
    "        # Look for the \"Overview\" heading\n",
    "        overview_heading = soup.find('h3', string=re.compile(r'Overview', re.I))\n",
    "        if overview_heading:\n",
    "            content = []\n",
    "            # Get all paragraphs until the next h3\n",
    "            for sibling in overview_heading.find_next_siblings():\n",
    "                if sibling.name == 'h3':\n",
    "                    break\n",
    "                if sibling.name == 'p':\n",
    "                    text = sibling.get_text(strip=True)\n",
    "                    if text:\n",
    "                        content.append(text)\n",
    "            return ' '.join(content)\n",
    "        return ''\n",
    "    \n",
    "    def _extract_research(self, soup: BeautifulSoup) -> List[Dict]:\n",
    "        \"\"\"Extract research findings for specific conditions.\"\"\"\n",
    "        # Look for \"What the research says\" heading\n",
    "        research_heading = soup.find('h3', string=re.compile(r'What the research says', re.I))\n",
    "        findings = []\n",
    "        \n",
    "        if research_heading:\n",
    "            # Get the next sibling (usually contains the list)\n",
    "            next_elem = research_heading.find_next_sibling()\n",
    "            \n",
    "            if next_elem and next_elem.name == 'ul':\n",
    "                # Each <li> contains a condition study\n",
    "                for li in next_elem.find_all('li', recursive=False):\n",
    "                    # The condition is in bold/strong at the start\n",
    "                    strong_tag = li.find(['strong', 'b'])\n",
    "                    if strong_tag:\n",
    "                        condition = strong_tag.get_text(strip=True).rstrip('.')\n",
    "                        # Get the full text and remove the condition name\n",
    "                        full_text = li.get_text(strip=True)\n",
    "                        evidence = full_text.replace(condition, '', 1).strip()\n",
    "                        \n",
    "                        findings.append({\n",
    "                            'condition': condition,\n",
    "                            'evidence': evidence\n",
    "                        })\n",
    "            \n",
    "            # Also check for paragraphs after the heading (some pages use this format)\n",
    "            if not findings:\n",
    "                for sibling in research_heading.find_next_siblings():\n",
    "                    if sibling.name == 'h3':\n",
    "                        break\n",
    "                    if sibling.name == 'p':\n",
    "                        text = sibling.get_text(strip=True)\n",
    "                        # Try to extract condition from bold text\n",
    "                        bold = sibling.find(['strong', 'b'])\n",
    "                        if bold:\n",
    "                            condition = bold.get_text(strip=True).rstrip('.')\n",
    "                            evidence = text.replace(condition, '', 1).strip()\n",
    "                            findings.append({\n",
    "                                'condition': condition,\n",
    "                                'evidence': evidence\n",
    "                            })\n",
    "        \n",
    "        return findings\n",
    "    \n",
    "    def _extract_safety_rating(self, soup: BeautifulSoup) -> Dict:\n",
    "        \"\"\"Extract Mayo Clinic's safety rating (green/yellow/red light).\"\"\"\n",
    "        # Look for \"Our take\" heading\n",
    "        our_take = soup.find('h3', string=re.compile(r'Our take', re.I))\n",
    "        rating = {'rating': '', 'explanation': ''}\n",
    "        \n",
    "        if our_take:\n",
    "            # Look for the rating in h4 (e.g., \"Generally safe\")\n",
    "            rating_h4 = our_take.find_next_sibling('h4')\n",
    "            if rating_h4:\n",
    "                rating_text = rating_h4.get_text(strip=True)\n",
    "                \n",
    "                # Determine color based on text\n",
    "                if 'generally safe' in rating_text.lower() or 'safe' in rating_text.lower():\n",
    "                    rating['rating'] = f'Green Light - {rating_text}'\n",
    "                elif 'caution' in rating_text.lower():\n",
    "                    rating['rating'] = f'Yellow Light - {rating_text}'\n",
    "                elif 'not recommended' in rating_text.lower():\n",
    "                    rating['rating'] = f'Red Light - {rating_text}'\n",
    "                else:\n",
    "                    rating['rating'] = rating_text\n",
    "            \n",
    "            # Get explanation from next paragraph\n",
    "            for sibling in our_take.find_next_siblings():\n",
    "                if sibling.name == 'h3':\n",
    "                    break\n",
    "                if sibling.name == 'p':\n",
    "                    text = sibling.get_text(strip=True)\n",
    "                    if text and len(text) > 20:\n",
    "                        rating['explanation'] = text\n",
    "                        break\n",
    "        \n",
    "        return rating\n",
    "    \n",
    "    def _extract_side_effects(self, soup: BeautifulSoup) -> List[str]:\n",
    "        \"\"\"Extract side effects and adverse reactions.\"\"\"\n",
    "        # Look for \"Safety and side effects\" heading\n",
    "        safety_heading = soup.find('h3', string=re.compile(r'Safety and side effects', re.I))\n",
    "        side_effects = []\n",
    "        \n",
    "        if safety_heading:\n",
    "            for sibling in safety_heading.find_next_siblings():\n",
    "                if sibling.name == 'h3':\n",
    "                    break\n",
    "                \n",
    "                # Look for unordered lists\n",
    "                if sibling.name == 'ul':\n",
    "                    for li in sibling.find_all('li', recursive=False):\n",
    "                        effect = li.get_text(strip=True)\n",
    "                        if effect:\n",
    "                            side_effects.append(effect)\n",
    "                \n",
    "                # Also check paragraphs for side effects\n",
    "                elif sibling.name == 'p':\n",
    "                    text = sibling.get_text(strip=True)\n",
    "                    # Keep informative paragraphs about side effects\n",
    "                    if text and ('can cause' in text.lower() or 'side effect' in text.lower() or \n",
    "                                'might include' in text.lower() or 'may cause' in text.lower()):\n",
    "                        side_effects.append(text)\n",
    "        \n",
    "        return side_effects\n",
    "    \n",
    "    def _extract_interactions(self, soup: BeautifulSoup) -> List[Dict]:\n",
    "        \"\"\"Extract drug-supplement interactions.\"\"\"\n",
    "        # Look for \"Interactions\" heading\n",
    "        interactions_heading = soup.find('h3', string=re.compile(r'Interactions', re.I))\n",
    "        interactions = []\n",
    "        \n",
    "        if interactions_heading:\n",
    "            # Check if there's a \"Possible drug interactions include:\" paragraph\n",
    "            intro_para = interactions_heading.find_next_sibling('p')\n",
    "            \n",
    "            # Look for the list of interactions\n",
    "            ul_elem = interactions_heading.find_next_sibling('ul')\n",
    "            if not ul_elem and intro_para:\n",
    "                ul_elem = intro_para.find_next_sibling('ul')\n",
    "            \n",
    "            if ul_elem:\n",
    "                for li in ul_elem.find_all('li', recursive=False):\n",
    "                    # The drug class is usually in bold/strong\n",
    "                    strong_tag = li.find(['strong', 'b'])\n",
    "                    if strong_tag:\n",
    "                        drug_class = strong_tag.get_text(strip=True).rstrip('.')\n",
    "                        # Get the full text and remove drug class\n",
    "                        full_text = li.get_text(strip=True)\n",
    "                        description = full_text.replace(drug_class, '', 1).strip()\n",
    "                        \n",
    "                        interactions.append({\n",
    "                            'drug_class': drug_class,\n",
    "                            'description': description,\n",
    "                            'severity': self._infer_severity(description)\n",
    "                        })\n",
    "        \n",
    "        return interactions\n",
    "    \n",
    "    def _extract_contraindications(self, soup: BeautifulSoup) -> List[str]:\n",
    "        \"\"\"Extract contraindications (when NOT to use).\"\"\"\n",
    "        contraindications = []\n",
    "        \n",
    "        # Check \"Safety and side effects\" section for \"don't use\" statements\n",
    "        safety_heading = soup.find('h3', string=re.compile(r'Safety and side effects', re.I))\n",
    "        if safety_heading:\n",
    "            for sibling in safety_heading.find_next_siblings():\n",
    "                if sibling.name == 'h3':\n",
    "                    break\n",
    "                \n",
    "                if sibling.name == 'p':\n",
    "                    text = sibling.get_text(strip=True)\n",
    "                    text_lower = text.lower()\n",
    "                    # Look for contraindication phrases\n",
    "                    if any(phrase in text_lower for phrase in [\"don't use\", \"do not use\", \"avoid\", \n",
    "                                                                 \"should not\", \"shouldn't\"]):\n",
    "                        contraindications.append(text)\n",
    "        \n",
    "        return contraindications\n",
    "    \n",
    "    def _extract_usage_notes(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Extract general usage notes and recommendations.\"\"\"\n",
    "        notes = []\n",
    "        \n",
    "        # Check \"Our take\" section\n",
    "        our_take = soup.find('h3', string=re.compile(r'Our take', re.I))\n",
    "        if our_take:\n",
    "            for sibling in our_take.find_next_siblings():\n",
    "                if sibling.name == 'h3':\n",
    "                    break\n",
    "                if sibling.name == 'p':\n",
    "                    text = sibling.get_text(strip=True)\n",
    "                    if text and len(text) > 30:\n",
    "                        notes.append(text)\n",
    "        \n",
    "        return ' '.join(notes)\n",
    "    \n",
    "    def _infer_severity(self, description: str) -> str:\n",
    "        \"\"\"Infer interaction severity from description text.\"\"\"\n",
    "        text_lower = description.lower()\n",
    "        \n",
    "        if any(word in text_lower for word in ['serious', 'severe', 'dangerous', 'life-threatening', 'major']):\n",
    "            return 'Major'\n",
    "        elif any(word in text_lower for word in ['moderate', 'increase', 'worsen', 'reduce effectiveness']):\n",
    "            return 'Moderate'\n",
    "        else:\n",
    "            return 'Minor'\n",
    "    \n",
    "    def scrape_all_supplements(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrape all supplements from the main page and return as DataFrame.\n",
    "        First fetches all supplement URLs, then scrapes each one.\n",
    "        \n",
    "        Returns:\n",
    "            pandas DataFrame with all supplement data\n",
    "        \"\"\"\n",
    "        # First, get all the supplement links\n",
    "        if not self.supplement_urls:\n",
    "            self.get_supplement_links()\n",
    "            time.sleep(self.delay)  # Be polite after fetching main page\n",
    "        \n",
    "        if not self.supplement_urls:\n",
    "            print(\"Error: No supplement URLs found. Cannot proceed.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        all_data = []\n",
    "        total = len(self.supplement_urls)\n",
    "        \n",
    "        for idx, (name, url) in enumerate(self.supplement_urls.items(), 1):\n",
    "            print(f\"[{idx}/{total}] \", end=\"\")\n",
    "            data = self.scrape_supplement(name, url)\n",
    "            if data:\n",
    "                all_data.append(data)\n",
    "            \n",
    "            # Be polite - wait between requests\n",
    "            if idx < total:  # Don't wait after the last one\n",
    "                time.sleep(self.delay)\n",
    "        \n",
    "        print(f\"\\nSuccessfully scraped {len(all_data)}/{total} supplements\")\n",
    "        return pd.DataFrame(all_data)\n",
    "    \n",
    "    def save_to_csv(self, df: pd.DataFrame, filename: str = 'mayo_clinic_supplements_raw.csv'):\n",
    "        \"\"\"Save scraped data to CSV file.\"\"\"\n",
    "        # Convert list/dict columns to strings for CSV compatibility\n",
    "        df_copy = df.copy()\n",
    "        for col in df_copy.columns:\n",
    "            if df_copy[col].dtype == 'object':\n",
    "                df_copy[col] = df_copy[col].apply(lambda x: str(x) if isinstance(x, (list, dict)) else x)\n",
    "        \n",
    "        df_copy.to_csv(filename, index=False, encoding='utf-8')\n",
    "        print(f\"Data saved to {filename}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize scraper with 2-second delay between requests\n",
    "    scraper = MayoClinicScraper(delay=2.0)\n",
    "    \n",
    "    # First, get all supplement links from the main page\n",
    "    print(\"Step 1: Fetching supplement URLs...\")\n",
    "    print(\"=\"*50)\n",
    "    links = scraper.get_supplement_links()\n",
    "    \n",
    "    if links:\n",
    "        print(\"\\nFound supplements:\")\n",
    "        for name, url in list(links.items())[:5]:  # Show first 5\n",
    "            print(f\"  - {name}: {url}\")\n",
    "        print(f\"  ... and {len(links)-5} more\\n\")\n",
    "    \n",
    "    # Option 1: Scrape a single supplement for testing\n",
    "    print(\"=\"*50)\n",
    "    print(\"Step 2: Testing with single supplement (Melatonin):\")\n",
    "    print(\"=\"*50)\n",
    "    if 'Melatonin' in scraper.supplement_urls:\n",
    "        melatonin_data = scraper.scrape_supplement('Melatonin')\n",
    "        if melatonin_data:\n",
    "            print(\"\\nSample data structure:\")\n",
    "            for key, value in melatonin_data.items():\n",
    "                if isinstance(value, str):\n",
    "                    preview = value[:100] + \"...\" if len(value) > 100 else value\n",
    "                    print(f\"{key}: {preview}\")\n",
    "                else:\n",
    "                    print(f\"{key}: {type(value).__name__} with {len(value) if hasattr(value, '__len__') else 'N/A'} items\")\n",
    "    \n",
    "    #Option 2: Scrape all supplements (uncomment to run)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Step 3: Scraping all supplements...\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    df = scraper.scrape_all_supplements()\n",
    "    scraper.save_to_csv(df)\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df[['name', 'category', 'safety_rating']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d760541f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
